<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Interview Experience</title>
  <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
  <style>
    .recording-icon {
      display: none;
    }
    .recording .recording-icon {
      display: block;
    }
  </style>
</head>
<body class="bg-gray-100 h-screen flex flex-col">

<!-- Top bar for the current question title -->
<header id="questionBar" class="bg-gray-800 text-white p-4 text-lg"></header>

<!-- Main container with left (voice visual) and right (video) -->
<main id="container" class="flex flex-1">
  <!-- Left Panel: Voice Visualizer -->
  <section id="leftPanel" class="flex-1 bg-gray-900 flex justify-center items-center">
    <canvas id="voiceVisualizer" class="w-full h-full"></canvas>
  </section>

  <!-- Right Panel: Video feed + overlay controls -->
  <section id="rightPanel" class="flex-1 bg-gray-800 relative flex justify-center items-center">
    <video id="videoElement" autoplay playsinline muted class="w-full h-full object-cover"></video>
    <div class="absolute top-4 right-4 recording-icon bg-red-600 text-white py-1 px-3 rounded-full">Recording...</div>
    <div class="absolute bottom-4 right-4 flex flex-col space-y-4">
      <button id="startBtn" class="bg-blue-500 text-white py-3 px-6 rounded text-lg">Start</button>
      <button id="stopBtn" class="bg-red-500 text-white py-3 px-6 rounded text-lg">Stop</button>
      <button id="nextQuestionBtn" class="bg-green-500 text-white py-3 px-6 rounded text-lg">Next</button>
    </div>
  </section>
</main>

<script>
  /****************************************************
   * 1. QUESTIONS SETUP
   ****************************************************/
  const questions = {{ questions | tojson }};

  let currentQuestionIndex = 0;

  // DOM References
  const questionBar = document.getElementById('questionBar');
  const startBtn = document.getElementById('startBtn');
  const stopBtn = document.getElementById('stopBtn');
  const nextQuestionBtn = document.getElementById('nextQuestionBtn');

  const videoElement = document.getElementById('videoElement');
  const voiceCanvas = document.getElementById('voiceVisualizer');
  const leftPanel = document.getElementById('leftPanel');
  const canvasCtx = voiceCanvas.getContext('2d');
  const rightPanel = document.getElementById('rightPanel');

  // Media / Recording Variables
  let audioContext, analyser, micSource;
  let mediaStream = null;
  let mediaRecorder = null;
  let recordedChunks = [];

  // Generate a random submission_id when the page loads
  const submissionId = {{ submission_id }};
  console.log("Submission ID:", submissionId);

  /****************************************************
   * 2. LOAD FIRST QUESTION
   ****************************************************/
  function loadQuestion(index) {
    if (index < questions.length) {
      questionBar.textContent = `Q${index + 1}: ${questions[index].title} (question_id: ${questions[index].id} (submission_id: ${submissionId})`;
    } else {
      questionBar.textContent = "No more questions.";
    }
  }
  loadQuestion(currentQuestionIndex);

  /****************************************************
   * 3. RESIZE CANVAS TO MATCH PANEL
   ****************************************************/
  function resizeCanvas() {
    voiceCanvas.width = leftPanel.clientWidth;
    voiceCanvas.height = leftPanel.clientHeight;
  }
  window.addEventListener('resize', resizeCanvas);
  window.addEventListener('load', resizeCanvas);

  /****************************************************
   * 4. AUDIO VISUALIZATION
   ****************************************************/
  function drawVoiceCircle() {
    if (!analyser) return;
    requestAnimationFrame(drawVoiceCircle);

    const dataArray = new Uint8Array(analyser.frequencyBinCount);
    analyser.getByteTimeDomainData(dataArray);

    let sum = 0;
    for (let i = 0; i < dataArray.length; i++) {
      const val = (dataArray[i] - 128) / 128;
      sum += (val * val);
    }
    const amplitude = Math.sqrt(sum / dataArray.length);

    canvasCtx.clearRect(0, 0, voiceCanvas.width, voiceCanvas.height);
    const centerX = voiceCanvas.width / 2;
    const centerY = voiceCanvas.height / 2;
    const maxRadius = Math.min(centerX, centerY) - 20;
    const radius = amplitude * maxRadius + 20;

    canvasCtx.beginPath();
    canvasCtx.arc(centerX, centerY, radius, 0, 2 * Math.PI);
    canvasCtx.fillStyle = '#ff5722';
    canvasCtx.fill();
  }

  /****************************************************
   * 5. START INTERVIEW (MEDIA CAPTURE + RECORDING)
   ****************************************************/
  async function startInterview() {
    try {
      // Prepare MediaRecorder to capture
      mediaRecorder = new MediaRecorder(mediaStream, { mimeType: "video/webm; codecs=vp8" });
      recordedChunks = [];

      mediaRecorder.ondataavailable = (e) => {
        if (e.data.size > 0) {
          recordedChunks.push(e.data);
        }
      };

      mediaRecorder.onstop = async () => {
        // After stopping, we have a complete video in 'recordedChunks'
        const blob = new Blob(recordedChunks, { type: "video/webm" });
        // Upload to /submit_answer
        const questionId = questions[currentQuestionIndex].id;
        await uploadAnswer(blob, submissionId, questionId);
        rightPanel.classList.remove('recording');
      };

      mediaRecorder.start();
      rightPanel.classList.add('recording');
      console.log("Recording started.");
    } catch (err) {
      console.error("Error starting media capture:", err);
    }
  }

  /****************************************************
   * 6. STOP INTERVIEW (STOP RECORDING)
   ****************************************************/
  function stopInterview() {
    if (!mediaRecorder) return;

    // Stop recording
    mediaRecorder.stop();
    console.log("Recording stopped.");
  }

  /****************************************************
   * 7. UPLOAD THE BLOB TO /submit_answer
   ****************************************************/
  async function uploadAnswer(blob, submissionId, questionId) {
    try {
      const formData = new FormData();
      // Name the file with question ID for clarity
      formData.append("file", blob, `question_${questionId}.webm`);
      formData.append("submission_id", submissionId);
      formData.append("question_id", questionId);

      // POST to your backend endpoint that handles S3 upload + DB
      const response = await fetch("/submit_answer", {
        method: "POST",
        body: formData
      });

      if (!response.ok) {
        console.error("Upload failed:", response.statusText);
        return;
      }

      const result = await response.json();
      console.log("Upload success:", result);
      // e.g. result might contain { "s3_url": "...", "transcript": "..." }
      // you could display or store the transcript, if included
    } catch (err) {
      console.error("Error uploading video:", err);
    }
  }

  /****************************************************
   * 8. NEXT QUESTION
   ****************************************************/
  function nextQuestion() {
    currentQuestionIndex++;
    if (currentQuestionIndex < questions.length) {
      loadQuestion(currentQuestionIndex);
    } else {
      questionBar.textContent = "Interview Completed!";
    }
  }

  /****************************************************
   * 9. EVENT LISTENERS
   ****************************************************/
  startBtn.addEventListener('click', startInterview);
  stopBtn.addEventListener('click', stopInterview);
  nextQuestionBtn.addEventListener('click', nextQuestion);

  /****************************************************
   * 10. INITIALIZE VIDEO STREAM
   ****************************************************/
  async function initializeVideoStream() {
    try {
      // Request audio + video from user
      mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true, video: true });
      videoElement.srcObject = mediaStream;

      // Audio visualization
      audioContext = new AudioContext();
      micSource = audioContext.createMediaStreamSource(mediaStream);
      analyser = audioContext.createAnalyser();
      analyser.fftSize = 256;
      micSource.connect(analyser);
      drawVoiceCircle();
    } catch (err) {
      console.error("Error initializing video stream:", err);
    }
  }

  // Initialize video stream on page load
  window.addEventListener('load', initializeVideoStream);

</script>
</body>
</html>
